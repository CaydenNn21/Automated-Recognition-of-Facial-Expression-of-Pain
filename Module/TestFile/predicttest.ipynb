{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code Here\n",
    "import os\n",
    "import cv2 as cv\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "from face_alignment import FaceAlignment\n",
    "from face_alignment import LandmarksType\n",
    "from preprocessData import preprocess\n",
    "from keras.models import load_model\n",
    "\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "class F1Score(tf.keras.metrics.Metric):\n",
    "    def __init__(self, name='f1_score', **kwargs):\n",
    "        super(F1Score, self).__init__(name=name, **kwargs)\n",
    "        self.true_positives = self.add_weight(name='true_positives', initializer='zeros')\n",
    "        self.false_positives = self.add_weight(name='false_positives', initializer='zeros')\n",
    "        self.false_negatives = self.add_weight(name='false_negatives', initializer='zeros')\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_true = tf.math.argmax(y_true, axis=1)  # Convert one-hot encoded to class indices\n",
    "        y_pred = tf.math.argmax(y_pred, axis=1)\n",
    "\n",
    "        true_positives = tf.reduce_sum(tf.cast(tf.logical_and(tf.equal(y_true, 1), tf.equal(y_pred, 1)), tf.float32))\n",
    "        false_positives = tf.reduce_sum(tf.cast(tf.logical_and(tf.equal(y_true, 0), tf.equal(y_pred, 1)), tf.float32))\n",
    "        false_negatives = tf.reduce_sum(tf.cast(tf.logical_and(tf.equal(y_true, 1), tf.equal(y_pred, 0)), tf.float32))\n",
    "\n",
    "        self.true_positives.assign_add(true_positives)\n",
    "        self.false_positives.assign_add(false_positives)\n",
    "        self.false_negatives.assign_add(false_negatives)\n",
    "\n",
    "    def result(self):\n",
    "        precision = self.true_positives / (self.true_positives + self.false_positives + tf.keras.backend.epsilon())\n",
    "        recall = self.true_positives / (self.true_positives + self.false_negatives + tf.keras.backend.epsilon())\n",
    "\n",
    "        f1 = 2 * (precision * recall) / (precision + recall + tf.keras.backend.epsilon())\n",
    "        return f1\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.true_positives.assign(0)\n",
    "        self.false_positives.assign(0)\n",
    "        self.false_negatives.assign(0)\n",
    "\n",
    "def capture_n_display():\n",
    "    cap = cv.VideoCapture(0)\n",
    "\n",
    "    # Face detector option can be blazeface, sfd, or dlib (must install with visual studio C++)\n",
    "    model = FaceAlignment(landmarks_type= LandmarksType.TWO_D, face_detector='blazeface', face_detector_kwargs={'back_model': True},device='cpu')\n",
    "    total_frames = int (5*30)\n",
    "    frames = [None]*total_frames\n",
    "    count = 0 \n",
    "    while count < total_frames:\n",
    "        ret, frame = cap.read()\n",
    "        frames[count] = frame\n",
    "        count += 1\n",
    "        cv.imshow('OpenCv',frame)\n",
    "        if cv.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv.destroyAllWindows()\n",
    "    return frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from face_alignment import FaceAlignment\n",
    "from face_alignment import LandmarksType\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "import torch\n",
    "from torchvision.transforms import functional as TF\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "class preprocess():\n",
    "    def __init__(self, frames):\n",
    "        super(preprocess, self).__init__()\n",
    "        print(\"preprocessing...\")\n",
    "        self.frames = frames\n",
    "        \n",
    "        # Select 21 frames from the video sequence for prediction\n",
    "        self.frames = self.selectFrame()\n",
    "\n",
    "        # Save the selected frames to a folder\n",
    "        self.saveFramestoFiles()\n",
    "\n",
    "        # Do landmark detection on the input frames for face recognition proposes\n",
    "        self.landmarkDetection()\n",
    "\n",
    "        # Mask the non-face area with black pixels\n",
    "        self.frames = self.maskFace()\n",
    "\n",
    "        # Tilt and align the face at centre, then crop the frames according to the face region\n",
    "        self.frames = self.tiltAlign()\n",
    "\n",
    "        for i in range (len(self.frames)):\n",
    "            cv.imwrite(f'{\"rawFrames\"}\\cropped_frame_{i}.png', cv.cvtColor(self.frames[i], cv.COLOR_BGR2RGB))\n",
    "\n",
    "        # self.tensor = self.padding_normalization(24)\n",
    "        self.normalized_img()\n",
    "        \n",
    "\n",
    "\n",
    "    def landmarkDetection(self):\n",
    "        frames = self.frames\n",
    "        output = []\n",
    "        framesLandmark = []\n",
    "        model = FaceAlignment(landmarks_type=LandmarksType.TWO_D, face_detector='blazeface',\n",
    "                              face_detector_kwargs={'back_model': True}, device='cpu')\n",
    "        for n in range(0, len(frames)):\n",
    "            img = (frames[n])\n",
    "            img = img.copy()\n",
    "            landmarks = model.get_landmarks(img)\n",
    "            landmarks_tuple = []\n",
    "            if landmarks is not None:\n",
    "                # Iterate over the detected faces\n",
    "                for pred in landmarks:\n",
    "                    # Draw landmarks on the frame\n",
    "                    for point in pred:\n",
    "                        x, y = point\n",
    "                        landmarks_tuple.append((int(x), int(y)))\n",
    "                        if 0 <= x < img.shape[1] and 0 <= y < img.shape[0]:\n",
    "                            cv.circle(img, (int(x), int(y)), 2, (0, 255, 0), -1)\n",
    "\n",
    "            framesLandmark.append(landmarks_tuple)\n",
    "            output.append(img)\n",
    "        self.framesLandmark = framesLandmark\n",
    "\n",
    "    def tiltAlign(self):\n",
    "        frames = self.frames\n",
    "        output =[]\n",
    "        for i in range(len(frames)):\n",
    "            img = frames[i]\n",
    "            landmarkTuple = self.framesLandmark[i]\n",
    "            # Landmark index of reight eye and left eye are\n",
    "            right_eye_cood = [(landmarkTuple[39][0] + landmarkTuple[36][0])/2, (landmarkTuple[39][1] + landmarkTuple[36][1])/2]\n",
    "            left_eye_cood = [(landmarkTuple[45][0] + landmarkTuple[42][0])/2, (landmarkTuple[45][1] + landmarkTuple[42][1])/2]\n",
    "            x1, y1 = right_eye_cood\n",
    "            x2, y2 = left_eye_cood\n",
    "\n",
    "            a = abs(y1 - y2)\n",
    "            b = abs(x2 - x1)\n",
    "            c = math.sqrt(a * a + b * b)\n",
    "\n",
    "            cos_alpha = (b * b + c * c - a * a) / (2 * b * c)\n",
    "\n",
    "            alpha = np.arccos(cos_alpha)\n",
    "            alpha = (alpha * 180) / math.pi\n",
    "            img = Image.fromarray(img)\n",
    "            if y1>y2 :\n",
    "                alpha = -alpha\n",
    "            img = np.array(img.rotate(alpha))\n",
    "            output.append(img)\n",
    "        return output\n",
    "    \n",
    "    def maskFace(self):\n",
    "        routes = [i for i in range (16,-1,-1)] + [i for i in range (17,26+1)]\n",
    "        \n",
    "        frames = self.frames\n",
    "        output = []\n",
    "        for n in range(len(frames)):\n",
    "            routes_cod = []\n",
    "            mask = None\n",
    "            out = None\n",
    "            landmarks_tuple = self.framesLandmark[n]\n",
    "            img = (frames[n])\n",
    "            img = img.copy()\n",
    "            img2 = img.copy()\n",
    "            for i in range (0, len(routes)-1):\n",
    "                source_point = routes[i]\n",
    "                target_point = routes[i+1]\n",
    "                \n",
    "                source_cod = landmarks_tuple[source_point]\n",
    "                target_cod = landmarks_tuple[target_point]\n",
    "                routes_cod.append(source_cod)\n",
    "                cv.line(img, (source_cod), (target_cod),(255,255,255),2)\n",
    "\n",
    "            routes_cod = routes_cod+[routes_cod[0]]\n",
    "\n",
    "            mask = np.zeros((img.shape[0], img.shape[1]))\n",
    "            mask = cv.fillConvexPoly(mask, np.array(routes_cod),1)\n",
    "            mask = mask.astype(np.bool_)\n",
    "            out = np.zeros_like(img)\n",
    "            out[mask] = img2[mask]\n",
    "            # plt.imshow(cv.cvtColor(out, cv.COLOR_BGR2RGB))\n",
    "            output.append(cv.cvtColor(self.cropFaceArea(out, mask), cv.COLOR_BGR2RGB))\n",
    "        return output\n",
    "\n",
    "    def cropFaceArea(self, frame, mask):\n",
    "\n",
    "        gray = cv.cvtColor(frame,cv.COLOR_BGR2GRAY)\n",
    "        contours, _ = cv.findContours(gray, cv.RETR_EXTERNAL, cv.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "        # Get the bounding box of the largest contour\n",
    "        \n",
    "        largest_contour = max(contours, key=cv.contourArea)\n",
    "        x, y, w, h = cv.boundingRect(largest_contour)\n",
    "\n",
    "        # Crop the image to the size of the masked face\n",
    "        cropped_image = frame[y:y+h, x:x+w]\n",
    "\n",
    "        return cropped_image\n",
    "\n",
    "    def selectFrame(self):\n",
    "        frames = self.frames\n",
    "        return [frames[i] for i in range(0, 150, 7)]\n",
    "\n",
    "    def saveFramestoFiles(self):\n",
    "        frames = self.frames\n",
    "\n",
    "        if not os.path.exists(\"rawFrames\"):\n",
    "            os.mkdir(\"rawFrames\")\n",
    "\n",
    "        for i in range(len(frames)):\n",
    "            cv.imwrite(f'{\"rawFrames\"}\\selectedFrames_{i}.png', frames[i])\n",
    "\n",
    "\n",
    "\n",
    "    def padding_normalization(self, target_length):\n",
    "        \"\"\"\n",
    "        Preprocesses a sequence of images and pads them to a target length.\n",
    "\n",
    "        Args:\n",
    "            images (list): List of PIL images.\n",
    "            target_length (int): Desired length of the sequence after padding.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Tensor of preprocessed and padded images.\n",
    "        \"\"\"\n",
    "        # Resize the images to a consistent size\n",
    "        array_images = self.frames\n",
    "        images =[]\n",
    "\n",
    "        for image in array_images:\n",
    "            images.append((Image.fromarray(image)))\n",
    "\n",
    "        resized_images = [TF.resize((img), [150, 150]) for img in images]\n",
    "\n",
    "        # Convert the images to tensors\n",
    "        tensor_images = [TF.to_tensor(img) for img in resized_images]\n",
    "\n",
    "        # Stack the tensor images along a new dimension (sequence dimension)\n",
    "        tensor_sequence = torch.stack(tensor_images)\n",
    "\n",
    "        # Calculate the current length of the sequence\n",
    "        current_length = tensor_sequence.size(0)\n",
    "\n",
    "        # Pad the sequence if necessary\n",
    "        if current_length < target_length:\n",
    "            padding_length = target_length - current_length\n",
    "            padding = torch.zeros(padding_length, *tensor_sequence.shape[1:])\n",
    "            tensor_sequence = torch.cat((tensor_sequence, padding))\n",
    "\n",
    "        # Normalize the tensor sequence\n",
    "        # Define the mean and standard deviation values for normalization\n",
    "        mean = [0.485, 0.456, 0.406]\n",
    "        std = [0.229, 0.224, 0.225]\n",
    "\n",
    "        # Apply normalization to the tensor sequence\n",
    "        normalize = transforms.Normalize(mean=mean, std=std)\n",
    "        self.normalized_sequence = normalize(tensor_sequence)\n",
    "\n",
    "        return self.normalized_sequence\n",
    "    \n",
    "    def normalized_img(self):\n",
    "        frames_temp = []\n",
    "        for image in self.frames:\n",
    "            frames_temp.append(cv.resize(image, dsize=(150, 150), interpolation=cv.INTER_CUBIC))\n",
    "\n",
    "        all_frames32 = np.array(frames_temp, dtype=\"float32\")\n",
    "        # Normalize the frames\n",
    "        all_frames_1 = all_frames32/ 255.0\n",
    "        self.normalized_frames = all_frames_1\n",
    "        return all_frames_1\n",
    "\n",
    "    \n",
    "    def get_preprocessed_frames(self):\n",
    "        frames = self.normalized_frames\n",
    "        return frames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150\n",
      "[[[116 131 145]\n",
      "  [113 128 143]\n",
      "  [115 130 144]\n",
      "  ...\n",
      "  [127 129 142]\n",
      "  [127 129 142]\n",
      "  [126 128 141]]\n",
      "\n",
      " [[118 131 143]\n",
      "  [117 130 143]\n",
      "  [117 129 144]\n",
      "  ...\n",
      "  [126 128 139]\n",
      "  [126 128 139]\n",
      "  [125 127 138]]\n",
      "\n",
      " [[120 131 143]\n",
      "  [120 131 143]\n",
      "  [120 131 143]\n",
      "  ...\n",
      "  [126 129 136]\n",
      "  [125 128 135]\n",
      "  [124 127 133]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 43  47  58]\n",
      "  [ 42  47  58]\n",
      "  [ 43  47  58]\n",
      "  ...\n",
      "  [146 148 138]\n",
      "  [144 147 137]\n",
      "  [144 147 137]]\n",
      "\n",
      " [[ 41  46  55]\n",
      "  [ 42  46  55]\n",
      "  [ 44  46  55]\n",
      "  ...\n",
      "  [144 147 137]\n",
      "  [143 145 136]\n",
      "  [142 144 135]]\n",
      "\n",
      " [[ 44  46  55]\n",
      "  [ 42  45  54]\n",
      "  [ 41  44  53]\n",
      "  ...\n",
      "  [143 146 135]\n",
      "  [142 145 133]\n",
      "  [142 145 133]]]\n",
      "preprocessing...\n",
      "22\n",
      "1/1 [==============================] - 0s 407ms/step\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "start = None \n",
    "while True:\n",
    "    user_input = str(input(\"Start? (Y/n) :\")).lower()\n",
    "    start = True if user_input == \"y\" else False\n",
    "\n",
    "    if start:\n",
    "        # Call the appropriate function or perform the desired action\n",
    "        frames = capture_n_display()\n",
    "        break\n",
    "print(len(frames))\n",
    "print((frames[1]))\n",
    "# Preprocess each frame\n",
    "frame_preprocessing = preprocess(frames)\n",
    "\n",
    "x = frame_preprocessing.get_preprocessed_frames()\n",
    "\n",
    "print(len(x))\n",
    "\n",
    "# Replace 'your_model_path' with the actual path to your saved model file\n",
    "loaded_model = tf.keras.models.load_model(\"C:\\\\Users\\\\xiao cheng\\\\Downloads\\\\model_fold_1.h5\",\n",
    "                                      custom_objects={\"F1Score\": F1Score})\n",
    "\n",
    "pred = loaded_model.predict(x)\n",
    "predicted_classes = tf.argmax(pred, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = tf.make_ndarray(tf.make_tensor_proto(predicted_classes)).tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "print(max(set(test), key = test.count))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
